{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e229486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from math import sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13dc665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drelu_3 = (self.ReLU(self.outputs3, deriv=True)[0] * loss_deriv).sum(axis=1, keepdims=True)\n",
    "#         dinputs3 = drelu_3 * self.weights3.T # For each input to the neuron, show how the output changes\n",
    "#         dweights3 = (drelu_3 * self.inputs.T).sum(axis=1, keepdims=True)\n",
    "#         dbiases3 = drelu_3\n",
    "        \n",
    "#         self.weights3 -= (self.learning_rate * dweights3).T\n",
    "#         self.biases3 -= (self.learning_rate * dbiases3).T\n",
    "#         # ======================================\n",
    "#         drelu_2 = (self.ReLU(self.outputs2, deriv=True)[0] * dinputs3).sum(axis=1, keepdims=True)\n",
    "#         dinputs2 = drelu_2 * self.weights2.T # For each input to the neuron, show how the output changes\n",
    "#         dweights2 = (drelu_2 * self.inputs.T).sum(axis=1, keepdims=True)\n",
    "#         dbiases2 = drelu_2\n",
    "        \n",
    "#         self.weights2 -= (self.learning_rate * dweights2).T\n",
    "#         self.biases2 -= (self.learning_rate * dbiases2).T\n",
    "#         # =====================================\n",
    "#         drelu_1 = (self.ReLU(self.outputs1, deriv=True)[0] * dinputs2).sum(axis=1, keepdims=True)\n",
    "#         dinputs1 = drelu_1 * self.weights1.T # For each input to the neuron, show how the output changes\n",
    "#         dweights1 = (drelu_1 * self.inputs.T).sum(axis=1, keepdims=True)\n",
    "#         dbiases1 = drelu_1\n",
    "        \n",
    "#         self.weights1 -= (self.learning_rate * dweights1).T\n",
    "#         self.biases1 -= (self.learning_rate * dbiases1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "916afaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([\n",
    "    *[[num] for num in range (10000)]\n",
    "])\n",
    "\n",
    "targets = np.array([\n",
    "    *[[sin(num)] for num in range (10000)]\n",
    "])\n",
    "\n",
    "inputs_scaler = preprocessing.StandardScaler().fit(inputs)\n",
    "inputs = inputs_scaler.transform(inputs)\n",
    "\n",
    "targets_scaler = preprocessing.StandardScaler().fit(targets)\n",
    "targets = targets_scaler.transform(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9590aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    2 neuron in the first layer with 1 input each\n",
    "    2 neurons in the second layer with 2 input each\n",
    "    1 neuron in the third layer with 2 inputs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weights1 = np.random.rand(1, 2)\n",
    "        self.biases1 = np.random.rand(1, 2)\n",
    "    \n",
    "        self.weights2 = np.random.rand(2, 2)\n",
    "        self.biases2 = np.random.rand(1, 2)\n",
    "    \n",
    "        self.weights3 = np.random.rand(1, 2)\n",
    "        self.biases3 = np.random.rand(1, 1)\n",
    "        \n",
    "        self.learning_rate = 0.00001\n",
    "    \n",
    "    @staticmethod\n",
    "    def ReLU(x, deriv=False):\n",
    "        if deriv:\n",
    "            r = np.zeros_like(x)\n",
    "            r[x > 0] = 1\n",
    "            return r\n",
    "        \n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def MSE(predicted, targets, mean=True):\n",
    "        errors = np.sum( np.power( targets - predicted, 2) )/len(targets)\n",
    "        return errors.mean() if mean else errors\n",
    "    \n",
    "    @staticmethod\n",
    "    def L2Loss(predicted, targets, deriv=False):\n",
    "        if deriv:\n",
    "            return 2 * np.sum(predicted, axis=1, keepdims=True)\n",
    "        return np.sum( np.power(targets - predicted, 2) )\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        self.outputs1 = inputs @ self.weights1.T + self.biases1\n",
    "        self.values1 = self.ReLU(self.outputs1)\n",
    "        \n",
    "        self.outputs2 = self.values1 @ self.weights2.T + self.biases2\n",
    "        self.values2 = self.ReLU(self.outputs2)\n",
    "        \n",
    "        self.outputs3 = self.values2 @ self.weights3.T + self.biases3\n",
    "        self.values3 = self.ReLU(self.outputs3)\n",
    "        return self.values3\n",
    "    \n",
    "    def forward(self, inputs, targets, silent=False):\n",
    "        self.targets = targets\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        self.predict(inputs)\n",
    "        \n",
    "        mse = self.MSE(self.values3, targets)\n",
    "        loss = self.L2Loss(self.values3, targets)\n",
    "        \n",
    "        if not silent:\n",
    "            print(f'Mean Squared Error: {mse:.3f}')\n",
    "            print(f'L2 Loss: {loss:3f}')\n",
    "        \n",
    "    def backwards(self):\n",
    "        # For each sample (column), show how each neuron/output (row) changes the loss function value \n",
    "        loss_deriv = self.L2Loss(self.values3, self.targets, deriv=True).T # neurons x samples\n",
    "        \n",
    "        # ReLU deriv of 3 row shows how the ReLU function (output) of the neuron is affected by its inputs\n",
    "        # ReLU input is the sum of the products for each sample (column)\n",
    "        # This is the measure of how each neuron changes the loss ???\n",
    "        drelu_3 = (self.ReLU(self.outputs3, deriv=True)[0] * loss_deriv).sum(axis=1, keepdims=True)\n",
    "        dinputs3 = drelu_3 * self.weights3.T # For each input to the neuron, show how the output changes\n",
    "        dweights3 = (drelu_3 * self.inputs.T).sum(axis=1, keepdims=True)\n",
    "        dbiases3 = drelu_3\n",
    "        \n",
    "        self.weights3 -= (self.learning_rate * dweights3).T\n",
    "        self.biases3 -= (self.learning_rate * dbiases3).T\n",
    "        # ======================================\n",
    "        drelu_2 = (self.ReLU(self.outputs2, deriv=True)[0] * dinputs3).sum(axis=1, keepdims=True)\n",
    "        dinputs2 = drelu_2 * self.weights2.T # For each input to the neuron, show how the output changes\n",
    "        dweights2 = (drelu_2 * self.inputs.T).sum(axis=1, keepdims=True)\n",
    "        dbiases2 = drelu_2\n",
    "        \n",
    "        self.weights2 -= (self.learning_rate * dweights2).T\n",
    "        self.biases2 -= (self.learning_rate * dbiases2).T\n",
    "        # =====================================\n",
    "        drelu_1 = (self.ReLU(self.outputs1, deriv=True)[0] * dinputs2).sum(axis=1, keepdims=True)\n",
    "        dinputs1 = drelu_1 * self.weights1.T # For each input to the neuron, show how the output changes\n",
    "        dweights1 = (drelu_1 * self.inputs.T).sum(axis=1, keepdims=True)\n",
    "        dbiases1 = drelu_1\n",
    "        \n",
    "        self.weights1 -= (self.learning_rate * dweights1).T\n",
    "        self.biases1 -= (self.learning_rate * dbiases1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5d80031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before fitting: [[3.33426474]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\MIVCHE~1\\AppData\\Local\\Temp/ipykernel_21052/2333818064.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackwards\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\MIVCHE~1\\AppData\\Local\\Temp/ipykernel_21052/1934985332.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, targets, silent)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mmse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\MIVCHE~1\\AppData\\Local\\Temp/ipykernel_21052/1934985332.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 1)"
     ]
    }
   ],
   "source": [
    "network = NeuralNetwork()\n",
    "print('Before fitting:', network.predict([[1,5]]))\n",
    "\n",
    "for inp, answer in zip(inputs, targets):\n",
    "    network.forward(np.array(inp), np.array([answer]), silent=True)\n",
    "    network.backwards()\n",
    "    \n",
    "print('After fitting:', network.predict([[1,5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1bd648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5faf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cf7f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
