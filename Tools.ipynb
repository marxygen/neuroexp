{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a128fe1",
   "metadata": {},
   "source": [
    "# Tools\n",
    "This notebook contains some usefulclasses and functions coded so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04c64cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "120bdc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(object):\n",
    "    \"\"\"A class representing a Dense layer in the neural network\"\"\"\n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return max(0, x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(inputs):\n",
    "        \"\"\"Computes probabilities given the VALUES\"\"\"\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "        keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "        keepdims=True)\n",
    "        return probabilities\n",
    "    \n",
    "    @staticmethod\n",
    "    def _categorical_cross_entropy(predicted, correct):\n",
    "        # Number of samples in a batch\n",
    "        samples = len(predicted)\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        predicted_clipped = np.clip(predicted, 1e-7, 1 - 1e-7)\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(correct.shape) == 1:\n",
    "            correct_confidences = predicted_clipped[\n",
    "            range(samples),\n",
    "            correct\n",
    "            ]\n",
    "            # Mask values - only for one-hot encoded labels\n",
    "        elif len(correct.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "            predicted_clipped * correct,\n",
    "            axis=1\n",
    "            )\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return np.mean(negative_log_likelihoods)\n",
    "    \n",
    "    def _accuracy(predictions, targets):\n",
    "        # Calculate values along second axis (axis of index 1)\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        # If targets are one-hot encoded - convert them\n",
    "        if len(class_targets.shape) == 2:\n",
    "            targets = np.argmax(targets, axis=1)\n",
    "        # True evaluates to 1; False to 0\n",
    "        return np.mean(predictions==targets)\n",
    "    \n",
    "    \n",
    "    def __init__(self, inputs: int, neurons: int, activation):\n",
    "        # Initialize weights randomly\n",
    "        # Each COLUMN in the resulting matrix is a neuron's weights\n",
    "        # It is done to avoid transposing the weights matrix every time we make a forward pass\n",
    "        # np.random.randn produces a Gaussian distribution with mean of 0 and variance of 1\n",
    "        self.weights = 0.01 * np.random.randn(inputs, neurons)\n",
    "        # Biases default to zero\n",
    "        self.biases = np.zeros((1, neurons))\n",
    "        # Set activation function\n",
    "        if isinstance(activation, str):\n",
    "            if hasattr(self, activation):\n",
    "                self.activation = getattr(self, activation)\n",
    "            else:\n",
    "                raise ValueError(f'Invalid function \"{activation}\": no such function is defined! You might want to pass in the function object instead')\n",
    "        elif callable(activation):\n",
    "            self.activation = activation\n",
    "        else:\n",
    "            raise ValueError(f'\"Activation\" parameter must be either a string specifying activation function name or a callable')\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.output = self.activation(np.dot(inputs, self.weights) + self.biases)\n",
    "        return self.output\n",
    "    \n",
    "    @property\n",
    "    def accuracy(self, targets):\n",
    "        return self._accuracy(self.output, targets)\n",
    "    \n",
    "    @property\n",
    "    def crossentropy_loss(self, targets):\n",
    "        return self._categorical_cross_entropy(self.output, targets)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44fd930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers: List[Dense]):\n",
    "        self.layers = layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661fb18a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
