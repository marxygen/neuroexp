{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8bf5aac",
   "metadata": {},
   "source": [
    "# Simple one-neuron backpropagation\n",
    "To simplify things further, let's find the influence one of the inputs has on ReLU output of the neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fdbb388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05fa8518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron output: 6\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, -2, 3]\n",
    "\n",
    "bias = 1\n",
    "weights = [-3, -1, 2]\n",
    "\n",
    "def get_output(weights, bias):\n",
    "    xw0 = weights[0] * inputs[0]\n",
    "    xw1 = weights[1] * inputs[1]\n",
    "    xw2 = weights[2] * inputs[2]\n",
    "    z = xw0 + xw1 + xw2 + bias\n",
    "    # Apply ReLU\n",
    "    y = max(z, 0)\n",
    "    return y\n",
    "\n",
    "y = get_output(weights, bias)\n",
    "\n",
    "print('Neuron output:', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3f5dc0",
   "metadata": {},
   "source": [
    "## Version 1\n",
    "All calculations are performed explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2134bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve(weights, bias, z):\n",
    "    # Now start computing derivatives\n",
    "    # Suppose that we received dvalue from the previous layer\n",
    "    dvalue = 1\n",
    "\n",
    "    # Calculate the derivative of the ReLU w.r.t the sum\n",
    "    d_relu_d_sum = 1 if z else 0\n",
    "\n",
    "    # Calculate the derivative of sum w.r.t. multiplication\n",
    "    d_sum_d_mul = 1\n",
    "\n",
    "    # Calculate the derivative w.r.t bias\n",
    "    d_sum_d_bias = 1\n",
    "\n",
    "    # Calculate the derivative of multiplication w.r.t weight ...\n",
    "    d_mul_d_w1 = inputs[0]\n",
    "    d_mul_d_w2 = inputs[1]\n",
    "    d_mul_d_w3 = inputs[2]\n",
    "\n",
    "    # and input\n",
    "    d_mul_d_i1 = weights[0]\n",
    "    d_mul_d_i2 = weights[1]\n",
    "    d_mul_d_i3 = weights[2]\n",
    "\n",
    "    # Group derivatives\n",
    "    d_w1 = dvalue * d_relu_d_sum * d_sum_d_mul * d_mul_d_w1\n",
    "    d_w2 = dvalue * d_relu_d_sum * d_sum_d_mul * d_mul_d_w2\n",
    "    d_w3 = dvalue * d_relu_d_sum * d_sum_d_mul * d_mul_d_w3\n",
    "\n",
    "    d_i1 = dvalue * d_relu_d_sum * d_sum_d_mul * d_mul_d_i1\n",
    "    d_i2 = dvalue * d_relu_d_sum * d_sum_d_mul * d_mul_d_i2\n",
    "    d_i3 = dvalue * d_relu_d_sum * d_sum_d_mul * d_mul_d_i3\n",
    "\n",
    "    d_bias = dvalue * d_relu_d_sum * d_sum_d_bias\n",
    "\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    weights[0] -= learning_rate * d_w1\n",
    "    weights[1] -= learning_rate * d_w2\n",
    "    weights[2] -= learning_rate * d_w3\n",
    "\n",
    "    bias -= learning_rate * d_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a211ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before improvement: 6\n",
      "After \"epoch\" #1: 5.986\n",
      "After \"epoch\" #2: 5.972\n",
      "After \"epoch\" #3: 5.958\n",
      "After \"epoch\" #4: 5.944\n",
      "After \"epoch\" #5: 5.930\n",
      "After \"epoch\" #6: 5.916\n",
      "After \"epoch\" #7: 5.902\n",
      "After \"epoch\" #8: 5.888\n",
      "After \"epoch\" #9: 5.874\n",
      "After \"epoch\" #10: 5.860\n",
      "After \"epoch\" #11: 5.846\n",
      "After \"epoch\" #12: 5.832\n",
      "After \"epoch\" #13: 5.818\n",
      "After \"epoch\" #14: 5.804\n",
      "After \"epoch\" #15: 5.790\n",
      "After \"epoch\" #16: 5.776\n",
      "After \"epoch\" #17: 5.762\n",
      "After \"epoch\" #18: 5.748\n",
      "After \"epoch\" #19: 5.734\n",
      "After \"epoch\" #20: 5.720\n",
      "After \"epoch\" #21: 5.706\n",
      "After \"epoch\" #22: 5.692\n",
      "After \"epoch\" #23: 5.678\n",
      "After \"epoch\" #24: 5.664\n",
      "After \"epoch\" #25: 5.650\n",
      "After \"epoch\" #26: 5.636\n",
      "After \"epoch\" #27: 5.622\n",
      "After \"epoch\" #28: 5.608\n",
      "After \"epoch\" #29: 5.594\n",
      "After \"epoch\" #30: 5.580\n",
      "After \"epoch\" #31: 5.566\n",
      "After \"epoch\" #32: 5.552\n",
      "After \"epoch\" #33: 5.538\n",
      "After \"epoch\" #34: 5.524\n",
      "After \"epoch\" #35: 5.510\n",
      "After \"epoch\" #36: 5.496\n",
      "After \"epoch\" #37: 5.482\n",
      "After \"epoch\" #38: 5.468\n",
      "After \"epoch\" #39: 5.454\n",
      "After \"epoch\" #40: 5.440\n",
      "After \"epoch\" #41: 5.426\n",
      "After \"epoch\" #42: 5.412\n",
      "After \"epoch\" #43: 5.398\n",
      "After \"epoch\" #44: 5.384\n",
      "After \"epoch\" #45: 5.370\n",
      "After \"epoch\" #46: 5.356\n",
      "After \"epoch\" #47: 5.342\n",
      "After \"epoch\" #48: 5.328\n",
      "After \"epoch\" #49: 5.314\n",
      "After \"epoch\" #50: 5.300\n"
     ]
    }
   ],
   "source": [
    "z = get_output(weights, bias)\n",
    "print(f'Before improvement:', z)\n",
    "\n",
    "for i in range(50):\n",
    "    improve(weights, bias, z)\n",
    "    z2 = get_output(weights, bias)\n",
    "    print(f'After \"epoch\" #{i+1}: {z2:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c147e5fb",
   "metadata": {},
   "source": [
    "## Version 2\n",
    "Removed variables that are **always** 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddc82777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if we can improve some calculations\n",
    "def improve(weights, bias, z):\n",
    "    # Now start computing derivatives\n",
    "    # Suppose that we received dvalue from the previous layer\n",
    "    dvalue = 1\n",
    "\n",
    "    # Calculate the derivative of the ReLU w.r.t the sum\n",
    "    d_relu_d_sum = 1 if z else 0\n",
    "\n",
    "    # Calculate the derivative of multiplication w.r.t weight ...\n",
    "    d_mul_d_w1 = inputs[0]\n",
    "    d_mul_d_w2 = inputs[1]\n",
    "    d_mul_d_w3 = inputs[2]\n",
    "\n",
    "    # and input\n",
    "    d_mul_d_i1 = weights[0]\n",
    "    d_mul_d_i2 = weights[1]\n",
    "    d_mul_d_i3 = weights[2]\n",
    "\n",
    "    # Group derivatives\n",
    "    d_w1 = dvalue * d_relu_d_sum * d_mul_d_w1\n",
    "    d_w2 = dvalue * d_relu_d_sum * d_mul_d_w2\n",
    "    d_w3 = dvalue * d_relu_d_sum * d_mul_d_w3\n",
    "\n",
    "    d_i1 = dvalue * d_relu_d_sum * d_mul_d_i1\n",
    "    d_i2 = dvalue * d_relu_d_sum * d_mul_d_i2\n",
    "    d_i3 = dvalue * d_relu_d_sum * d_mul_d_i3\n",
    "\n",
    "    d_bias = dvalue * d_relu_d_sum\n",
    "\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    weights[0] -= learning_rate * d_w1\n",
    "    weights[1] -= learning_rate * d_w2\n",
    "    weights[2] -= learning_rate * d_w3\n",
    "\n",
    "    bias -= learning_rate * d_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf54b9d2",
   "metadata": {},
   "source": [
    "### About dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fd82626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If a is an N-D array and b is a 1-D array, it is a sum product over the last axis of a and b.\n",
    "\n",
    "If a is an N-D array and b is an M-D array (where M>=2), it is a sum product\n",
    "over the last axis of a and the second-to-last axis of b\n",
    "\"\"\"\n",
    "inputs = np.array([\n",
    "    [1., -2., 3.],\n",
    "    [-1., 2., -3.],\n",
    "])\n",
    "\n",
    "d_relu=np.array([\n",
    "    [6.],\n",
    "    [6.],\n",
    "])\n",
    "\n",
    "# We were calculating dweights - the derivative of activation function w.r.t. inputs.\n",
    "# If we have multiple inputs for current neuron we expect to see them as rows in our matrix\n",
    "\n",
    "# The derivative is calculated by multiplying each input by derivatives of the activation function OF EACH INPUT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3b7c51",
   "metadata": {},
   "source": [
    "ReLU function derivative returns 1 or 0 for each sum, so it returns a np.array with `n` rows and 1 column for `n` inputs. When we multiply it by the `dvalues` we get the following array as the value of `d_relu`:\n",
    "```Python\n",
    "[[6.]\n",
    " [0.]]\n",
    "```\n",
    "Each row corresponds to an input entry. Thus, to get the product of `d_relu` and `inputs` (the `dweights`) we have to multiply `d_relu` by transposed `inputs`, BUT according to the rules above (**numpy determines the shape by the first element**), we have to move inputs to the first place to be compliant with case 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e7459450",
   "metadata": {},
   "outputs": [],
   "source": [
    "dweights = np.dot(inputs.T, d_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0dcda3",
   "metadata": {},
   "source": [
    "# Version 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be60f39",
   "metadata": {},
   "source": [
    "Let's consider the most complex situation: we have **2 neurons**, each has **3 inputs** and we have **4 samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "0ca3a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([\n",
    "    [1.,   2.,  3.],\n",
    "    [4.,   5.,  6.],\n",
    "    [7.,   8.,  9.],\n",
    "    [10., 11., 12.]\n",
    "])\n",
    "\n",
    "weights = np.array([\n",
    "    [0.1, 0.1, 0.1],\n",
    "    [0.2, 0.2, 0.2]\n",
    "])\n",
    "\n",
    "biases = np.array([\n",
    "    [1, 1.01]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85effea",
   "metadata": {},
   "source": [
    "First, we perform **forward pass**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed95a24",
   "metadata": {},
   "source": [
    "Obtain the value of the neuron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "4795403b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.6 , 2.21],\n",
       "       [2.5 , 4.01],\n",
       "       [3.4 , 5.81],\n",
       "       [4.3 , 7.61]])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron_outputs = np.dot(inputs, weights.T) + biases\n",
    "neuron_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acb77c1",
   "metadata": {},
   "source": [
    "And apply activation function to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "152a6676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.6 , 2.21],\n",
       "       [2.5 , 4.01],\n",
       "       [3.4 , 5.81],\n",
       "       [4.3 , 7.61]])"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = np.maximum(neuron_outputs, 0)\n",
    "# Columns are neurons and rows are their outputs for each sample\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b0d485",
   "metadata": {},
   "source": [
    "Second, perform backward pass. For simplicity let's try to decrease the output of this neuron.\n",
    "We do not use the `dvalues` just yet!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94af52d",
   "metadata": {},
   "source": [
    "Find the derivative of the ReLU function, which is 1 for any positive value and 0 for anything equal to or less than 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "d3b94383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_relu = np.where(values > 0, 1, 0)\n",
    "d_relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1484d3",
   "metadata": {},
   "source": [
    "Finding `dweights` - the measure of how the changes in the weight affect the value of the neuron. Resulting array has the gradients for each neuron for each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "37bf5345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22., 22.],\n",
       "       [26., 26.],\n",
       "       [30., 30.]])"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dweights = np.dot(inputs.T, d_relu)\n",
    "dweights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9690c123",
   "metadata": {},
   "source": [
    "Let's calculate `dinputs` - the measure of how the value of the neuron changed when its input changed\n",
    "For each input (the row), columns contain the gradient in given sample<br>\n",
    "**I'm kinda uncertain about this**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "858f931d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3, 0.3, 0.3],\n",
       "       [0.3, 0.3, 0.3],\n",
       "       [0.3, 0.3, 0.3],\n",
       "       [0.3, 0.3, 0.3]])"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dinputs = np.dot(d_relu, weights)\n",
    "dinputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb7aff9",
   "metadata": {},
   "source": [
    "\\[**Something I'm uncertain about**\\] Since we need to pass `dinputs` to previous layer and this layer expects a 1D array where each column corresponds to the value of the gradient that signifies the change in layer output when the inputs change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "2e808f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2, 1.2, 1.2]])"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dinputs = dinputs.sum(axis=0, keepdims=True)\n",
    "dinputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9f3b67",
   "metadata": {},
   "source": [
    "Let's calculate `dbiases`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "e4aba53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbiases = d_relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cb522e",
   "metadata": {},
   "source": [
    "Let's summarize everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "aa07aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([\n",
    "    [1.,   2.,  3.],\n",
    "    [4.,   5.,  6.],\n",
    "    [7.,   8.,  9.],\n",
    "    [10., 11., 12.]\n",
    "], dtype='float32')\n",
    "\n",
    "weights = np.array([\n",
    "    [1, 1, 1],\n",
    "    [2, 2, 2]\n",
    "], dtype='float32')\n",
    "\n",
    "biases = np.array([\n",
    "    [1, 1.01]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "085194f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward():\n",
    "    neuron_outputs = np.dot(inputs, weights.T) + biases\n",
    "    values = np.maximum(neuron_outputs, 0)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "00cc8021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwards(learning_rate=0.001):\n",
    "    global weights, inputs, biases\n",
    "    \n",
    "    d_relu = np.where(values > 0, 1, 0)\n",
    "    dweights = np.dot(inputs.T, d_relu)\n",
    "    dbiases = d_relu\n",
    "    \n",
    "    weights -= (learning_rate * dweights).T\n",
    "    biases -= (learning_rate * dbiases).sum(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "71aeb305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial values:\n",
      " [[ 7.   13.01]\n",
      " [16.   31.01]\n",
      " [25.   49.01]\n",
      " [34.   67.01]]\n",
      "\n",
      "After step 1:\n",
      "[[ 6.83199997 12.84200044]\n",
      " [15.59799928 30.60799928]\n",
      " [24.36400003 48.37400003]\n",
      " [33.12999887 66.14000269]]\n",
      "\n",
      "After step 2:\n",
      "[[ 6.66399993 12.67400089]\n",
      " [15.19599952 30.20599857]\n",
      " [23.72800006 47.73800006]\n",
      " [32.25999965 65.26999774]]\n",
      "\n",
      "After step 3:\n",
      "[[ 6.49600037 12.50600037]\n",
      " [14.79399976 29.80400166]\n",
      " [23.09200009 47.10200391]\n",
      " [31.39000043 64.40000424]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Initial values:\\n', forward(), end='\\n\\n')\n",
    "\n",
    "for step in range(3):\n",
    "    backwards()\n",
    "    print(f'After step {step+1}:\\n{forward()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8ddb70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
