{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8bf5aac",
   "metadata": {},
   "source": [
    "# Simple one-neuron backpropagation\n",
    "To simplify things further, let's find the influence one of the inputs has on ReLU output of the neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2fdbb388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3b640d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#Let's consider our neuron as a big function\n",
    "# y = ReLU (\n",
    "#        sum( \n",
    "#            mul(\n",
    "#                    i_1,\n",
    "#                    w_1\n",
    "#                ),\n",
    "#                \n",
    "#            mul(\n",
    "#                    i_2,\n",
    "#                    w_2\n",
    "#                ),\n",
    "#            \n",
    "#            mul(\n",
    "#                i_3,\n",
    "#                w_3\n",
    "#            ),\n",
    "#            \n",
    "#            bias \n",
    "#        ) \n",
    "#    )\n",
    "#\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "88db4b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [1, 2, 3]\n",
    "inputs = [1,2,3]\n",
    "bias = 0.3\n",
    "\n",
    "xw1 = weights[0] * inputs[0]\n",
    "xw2 = weights[1] * inputs[1]\n",
    "xw3 = weights[2] * inputs[2]\n",
    "\n",
    "z = xw1 + xw2 + xw3 + bias\n",
    "\n",
    "y = max(z, 0)\n",
    "\n",
    "# Suppose that we have the gradient from the layer preceding our y\n",
    "dvalue = 1\n",
    "\n",
    "# The derivative of ReLU w.r.t computed value before ReLU (our sum)\n",
    "d_relu_d_output = dvalue * (1 if z > 0 else 0)\n",
    "\n",
    "d_relu_d_mul = d_relu_d_output * 1 # Since all other arguments except for the sum will be 0 and the partial derivative w.r.t the variable is 1\n",
    "\n",
    "d_relu_d_w0 = d_relu_d_mul * inputs[0]\n",
    " \n",
    "# =>\n",
    "d_relu_d_w0 = dvalue * (1 if z > 0 else 0) * 1 * inputs[0]\n",
    "# =>\n",
    "d_relu_d_w0 = dvalue * (1 if z > 0 else 0) * inputs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1ab00197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before applying \"optimizer\":\n",
      "\tWeights: [1, 2, 3]\n",
      "\tReLU output: 14.3\n",
      "\n",
      "After applying \"optimizer\":\n",
      "\tWeights: [0.999, 1.998, 2.997]\n",
      "\tReLU output: 14.286000000000001\n"
     ]
    }
   ],
   "source": [
    "# Now that we have the answer to \"How does the ReLU (output) of the function change when we change its weight\",\n",
    "# we can try and lower the resulting value of the function to see if it's working\n",
    "d_relu_d_w0 = dvalue * (1 if z > 0 else 0) * inputs[0]\n",
    "d_relu_d_w1 = dvalue * (1 if z > 0 else 0) * inputs[1]\n",
    "d_relu_d_w2 = dvalue * (1 if z > 0 else 0) * inputs[2]\n",
    "\n",
    "xw1 = weights[0] * inputs[0]\n",
    "xw2 = weights[1] * inputs[1]\n",
    "xw3 = weights[2] * inputs[2]\n",
    "z = xw1 + xw2 + xw3 + bias\n",
    "y = max(z, 0)\n",
    "\n",
    "print(f'Before applying \"optimizer\":\\n\\tWeights: {weights}\\n\\tReLU output: {y}\\n')\n",
    "\n",
    "weights[0] += -0.001 * d_relu_d_w0\n",
    "weights[1] += -0.001 * d_relu_d_w1\n",
    "weights[2] += -0.001 * d_relu_d_w2\n",
    "\n",
    "xw1 = weights[0] * inputs[0]\n",
    "xw2 = weights[1] * inputs[1]\n",
    "xw3 = weights[2] * inputs[2]\n",
    "z = xw1 + xw2 + xw3 + bias\n",
    "y = max(z, 0)\n",
    "\n",
    "print(f'After applying \"optimizer\":\\n\\tWeights: {weights}\\n\\tReLU output: {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "9ac413aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8     1.21    2.385 ]\n",
      " [ 9.6     2.42    4.77  ]\n",
      " [14.4     3.63    7.155 ]\n",
      " [12.      3.025   5.9625]]\n"
     ]
    }
   ],
   "source": [
    "# Passed in gradient from the next layer\n",
    "# For each of the three samples, it contains the\n",
    "# derivatives of subsequent neurons w.r.t their input (0, 1, 2) - the neurons in the current layer\n",
    "# Row - sample\n",
    "# Column - derivative w.r.t. the neuron\n",
    "dvalues = np.array([\n",
    "    [1., 1., 1.],\n",
    "    [2., 2., 2.],\n",
    "    [3., 3., 3.]\n",
    "])\n",
    "\n",
    "# We have 3 sets of inputs - samples\n",
    "inputs = np.array([\n",
    "    [1,      2,   3,  2.5],\n",
    "    #[2.,    5., -1.,    2],\n",
    "    #[-1.5, 2.7, 3.3, -0.8]\n",
    "])\n",
    "\n",
    "# We have 3 sets of weights - one set for each neuron\n",
    "# we have 4 inputs, thus 4 weights\n",
    "# recall that we keep weights transposed\n",
    "weights = np.array([\n",
    "    [0.2,     0.8,  -0.5,    1],\n",
    "    [0.5,   -0.91,  0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17,  0.87]\n",
    "]).T\n",
    "\n",
    "# One bias for each neuron\n",
    "# biases are the row vector with a shape (1, neurons)\n",
    "biases = np.array([\n",
    "    [2, 3, 0.5]\n",
    "])\n",
    "\n",
    "# Forward pass\n",
    "layer_outputs = np.dot(inputs, weights) + biases # Dense layer\n",
    "relu_outputs = np.maximum(0, layer_outputs) # ReLU activation\n",
    "\n",
    "# Let's optimize and test backpropagation here\n",
    "\n",
    "# ReLU derivative\n",
    "# from next layer passed to current layer during backpropagation\n",
    "drelu = relu_outputs.copy()\n",
    "drelu[layer_outputs <= 0] = 0\n",
    "\n",
    "# Dense layer\n",
    "# dinputs - multiply by weights\n",
    "dinputs = np.dot(drelu, weights.T)\n",
    "\n",
    "# dweights - multiply by inputs\n",
    "dweights = np.dot(inputs.T, drelu)\n",
    "\n",
    "print(dweights)\n",
    "\n",
    "# dbiases - sum values, do this over samples (first axis), keepdims\n",
    "# since this by default will produce a plain list -\n",
    "# we explained this in the chapter 4\n",
    "dbiases = np.sum(drelu, axis=0, keepdims=True)\n",
    "\n",
    "# Update parameters\n",
    "weights += -0.001 * dweights\n",
    "biases += -0.001 * dbiases\n",
    "\n",
    "# print(weights)\n",
    "# print(biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "fc405c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.8 ,  1.21,  2.38],\n",
       "       [ 9.6 ,  2.42,  4.77],\n",
       "       [14.4 ,  3.63,  7.15],\n",
       "       [12.  ,  3.02,  5.96]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = np.array([\n",
    "    [1,      2,   3,  2.5],\n",
    "    #[2.,    5., -1.,    2],\n",
    "    #[-1.5, 2.7, 3.3, -0.8]\n",
    "])\n",
    "\n",
    "# We have 3 sets of weights - one set for each neuron\n",
    "# we have 4 inputs, thus 4 weights\n",
    "# recall that we keep weights transposed\n",
    "weights = np.array([\n",
    "    [0.2,     0.8,  -0.5,    1],\n",
    "    [0.5,   -0.91,  0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17,  0.87]\n",
    "]).T\n",
    "\n",
    "# One bias for each neuron\n",
    "# biases are the row vector with a shape (1, neurons)\n",
    "biases = np.array([\n",
    "    [2, 3, 0.5]\n",
    "])\n",
    "\n",
    "# Since I do not know numpy (it's 05.02.2022, I probably have changed already), we're going to omit numpy for calculation\n",
    "# to calculate each gradient we'll need to multiply the derivative of activation function by each weight\n",
    "drelu = relu_outputs.copy()\n",
    "drelu[layer_outputs <= 0] = 0 # Get derivative of activation function\n",
    "\n",
    "dweights = []\n",
    "\n",
    "# For each sample\n",
    "for sample in inputs:\n",
    "    for neuron_weights, bias in zip(weights.T, biases[0]):\n",
    "        neuron_value = np.maximum(0, np.dot(sample, neuron_weights) + bias)\n",
    "        drelu = neuron_value if neuron_value > 0 else 0\n",
    "        \n",
    "        dweights.append([round(drelu * inp, 2) for inp in sample])\n",
    "\n",
    "dweights = np.array(dweights).T\n",
    "dweights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30324423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2c4ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
